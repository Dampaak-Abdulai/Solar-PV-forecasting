{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21107b2b",
   "metadata": {},
   "source": [
    "# EXPLANATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ffefad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cef368",
   "metadata": {},
   "outputs": [],
   "source": [
    "Solar_data = pd.read_csv(r'C:\\Users\\USER\\Desktop\\BUI PROJECT\\Bui_Data_P.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dd45e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Solar_data_P = Solar_data[['DATE','TIME','DIFFUSE','WIND DIRECTION','PANEL TEMPERATURE','GLOBAL IRRADIATION',\n",
    "                           'HUMIDITY','AMBIENT TEMPERATURE','DIRECT IRRADIANCE','WIND SPEED','POWER']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f35e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b807e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "Solar_data_P.corr() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e09c931",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(Solar_data_P.corr(),annot=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15416c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(Solar_data[['DATE','TIME','DIFFUSE','WIND DIRECTION','PANEL TEMPERATURE','GLOBAL IRRADIATION',\n",
    "                         'HUMIDITY','AMBIENT TEMPERATURE','DIRECTIRRADIANCE','WINDSPEED', 'POWER']],height=2.5) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd894fcd",
   "metadata": {},
   "source": [
    "# DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e959b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "Solar_data = pd.read_csv(r'C:\\Users\\USER\\Desktop\\BUI PROJECT\\Bui_Data_P.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7d2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "Solar_data.columns = ['DATE','TIME','Dif','Wd','Pw','Pt','Ghi','hum','At','Di','Ws','Mth'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c6b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Solar_data['DATE_C'] = pd.to_datetime(Solar_data['DATE']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374c484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = pd.date_range('25-04-2021','26-04-2021',freq='15min') \n",
    "tb = tb[:-1] \n",
    "ts = tb.strftime('%H:%M') block_dict = {} j = 1 \n",
    "\n",
    "for i in range(len(ts)):\n",
    "    block_dict[ts[i]]=j     \n",
    "    j += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c0ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Solar_data['K']= pd.to_datetime(Solar_data['TIME']).astype(str).apply(lambda x: block_dict[str(x)[-8:-3]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3f6e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Solar_data = Solar_data[['DATE_C','TIME','K','Dif','Wd','Pw','Pt','Ghi','hum','At','Di','Ws','Mth']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d77a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "Solar_data.columns = ['DATE','TIME','K','Dif','Wd','Pw','Pt','Ghi','hum','At','Di','Ws','Mth'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f399d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "Solar_data.drop('Mth',axis=1,inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9389115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solar_data['TIME_C'] = Solar_data['TIME'].apply(lambda x:str(x)[-19:-3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b2f237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solar_data.drop('TIME_C',axis=1,inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27f55df",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_list = Solar_data.groupby('K') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a80a981",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = list() for i in Solar_data['K'].unique():dfs.append(grouped_list.get_group(i)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab4ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "grp96 = grouped_list.get_group(96) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd599675",
   "metadata": {},
   "outputs": [],
   "source": [
    "SolarBui = pd.concat(dfs,ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b754cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bui_wd = Solar_data[['DATE','Wd']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2e4a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bui_data_input = Solar_data[['DATE','Wd','Pw','Pt','Ghi','At','Di','Ws']] \n",
    "Bui_data_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a85dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bui_data_input.index = pd.to_datetime(Bui_data_input['DATE'], format='%Y-%m-%d') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cddc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bui_data_input.drop('DATE',axis=1,inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1706b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bui_dataIn_reshaped = pd.DataFrame(data=Bui_data_input.values.reshape(int(Bui_data_input.shape[0] / 96), 96), \n",
    "index=[day for day in Bui_data_input.resample('D').mean().dropna().index]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4a8105",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([Bui_wd],order='C') data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf9398",
   "metadata": {},
   "source": [
    "# Wind Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d388866",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bui_wd.index = pd.to_datetime(Bui_wd['DATE'],format='%Y-%m-%d')\n",
    "Bui_wd_reshaped = pd.DataFrame(data=data2, index=[day for day in Bui_wd.resample('D').mean().dropna().index])\n",
    "#daily = Bui_wd.values.reshape(int(Bui_wd.shape[0] / 96), 96) \n",
    "data2 = np.array([daily],order='C') data.shape\n",
    "data2.resize((339,96)) \n",
    "DF = pd.DataFrame(data2)\n",
    "grp1.index = pd.to_datetime(grp1['DATE'],format='%Y-%m-%d')\n",
    "Bui_wd_df.columns = ['wd' + str(i) for i in range(len(Bui_wd_df.columns))] \n",
    "Bui_wd_df.to_csv('Buiwd.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f85107",
   "metadata": {},
   "source": [
    "# Global Horizontal Irradiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec7db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bui_GHI = Solar_data[['DATE','Ghi']] \n",
    "Bui_GHI.index = pd.to_datetime(Bui_GHI['DATE'],format='%Y-%m-%d') \n",
    "Bui_GHI.drop('DATE',axis=1,inplace=True) \n",
    "daily2 = Bui_GHI.values.reshape(int(Bui_wd.shape[0] / 96), 96)\n",
    "dataghi = np.array([daily2],order='C') dataghi.shape\n",
    "dataghi.resize((339,96)) \n",
    "DF2 = pd.DataFrame(dataghi) \n",
    "Bui_GHI_df = pd.DataFrame(data=dataghi, index=grp1.index, columns=DF.columns)  \n",
    "Bui_GHI_df.columns = ['ghi' + str(i) for i in range(len(Bui_GHI_df.columns))] \n",
    "Bui_GHI_df.to_csv('BuiGHI.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271070d2",
   "metadata": {},
   "source": [
    "# Panel Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80feee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bui_Pt = Solar_data[['DATE','Pt']] \n",
    "Bui_Pt.index = pd.to_datetime(Bui_Pt['DATE'],format='%Y-%m-%d')\n",
    "Bui_Pt.drop('DATE',axis=1,inplace=True) \n",
    "daily3 = Bui_Pt.values.reshape(int(Bui_Pt.shape[0] / 96), 96) \n",
    "dataPt = np.array([daily3],order='C') dataPt.shape\n",
    "dataPt.resize((339,96)) \n",
    "DF2 = pd.DataFrame(dataPt) \n",
    "Bui_Pt_df = pd.DataFrame(data=dataPt, index=grp1.index, columns=DF.columns) \n",
    "Bui_Pt_df.columns = ['Pt' + str(i) for i in range(len(Bui_Pt_df.columns))] \n",
    "Bui_Pt_df.to_csv('BuiPt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76538d8d",
   "metadata": {},
   "source": [
    "# Ambient Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d149015",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bui_At = Solar_data[['DATE','At']]  \n",
    "Bui_At.index = pd.to_datetime(Bui_At['DATE'],format='%Y-%m-%d')  \n",
    "Bui_At.drop('DATE',axis=1,inplace=True) \n",
    "daily4 = Bui_At.values.reshape(int(Bui_At.shape[0] / 96), 96) \n",
    "dataAt = np.array([daily4],order='C') dataAt.shape\n",
    "dataAt.resize((339,96)) \n",
    "DF3 = pd.DataFrame(dataAt) \n",
    "Bui_At_df = pd.DataFrame(data=dataAt, index=grp1.index, columns=DF.columns) \n",
    "Bui_At_df.columns = ['At' + str(i) for i in range(len(Bui_At_df.columns))] \n",
    "Bui_At_df.to_csv('BuiAt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bcc445",
   "metadata": {},
   "source": [
    "# Direct Irradiance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb2499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bui_DI = Solar_data[['DATE','Di']] \n",
    "Bui_DI.index = pd.to_datetime(Bui_DI['DATE'],format='%Y-%m-%d' \n",
    "Bui_DI.drop('DATE',axis=1,inplace=True)  \n",
    "daily5 = Bui_DI.values.reshape(int(Bui_DI.shape[0] / 96), 96) \n",
    "dataDI = np.array([daily5],order='C') dataDI.shape\n",
    "dataDI.resize((339,96)) \n",
    "DF4 = pd.DataFrame(dataDI) \n",
    "Bui_DI_df = pd.DataFrame(data=dataDI, index=grp1.index, columns=DF.columns) \n",
    "Bui_DI_df.columns = ['Di' + str(i) for i in range(len(Bui_DI_df.columns))]  \n",
    "Bui_DI_df.to_csv('BuiAt.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa252c8",
   "metadata": {},
   "source": [
    "# Wind Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c7ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bui_WS = Solar_data[['DATE','Ws']]  \n",
    "Bui_WS.index = pd.to_datetime(Bui_WS['DATE'],format='%Y-%m-%d')  \n",
    "Bui_WS.drop('DATE',axis=1,inplace=True) \n",
    "daily6 = Bui_WS.values.reshape(int(Bui_WS.shape[0] / 96), 96) \n",
    "dataWS = np.array([daily6],order='C') dataWS.shape\n",
    "dataWS.resize((339,96)) \n",
    "DF4 = pd.DataFrame(dataWS) \n",
    "Bui_WS_df = pd.DataFrame(data=dataWS, index=grp1.index, columns=DF.columns) \n",
    "Bui_WS_df.columns = ['Ws' + str(i) for i in range(len(Bui_WS_df.columns))] \n",
    "Bui_WS_df.to_csv('BuiWS.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03549f91",
   "metadata": {},
   "source": [
    "# Train_data (df_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aa42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_inputs = pd.concat([Tp, Ip], axis=1, join='inner').truncate(before=pd.Timestamp('2020-0411'), #after=pd.Timestamp('2020-09-14')) \n",
    "df_inputs = pd.concat([Bui_At_df,Bui_DI_df,Bui_GHI_df,Bui_Pt_df,Bui_wd_df,Bui_WS_df],axis=1,join='inner') \n",
    "df_inputs.to_csv('df_inputs.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6142b3be",
   "metadata": {},
   "source": [
    "# Power output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e78bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bui_Pw = Solar_data[['DATE','Pw']] \n",
    "Bui_Pw.index = pd.to_datetime(Bui_Pw['DATE'],format='%Y-%m-%d') \n",
    "Bui_Pw.drop('DATE',axis=1,inplace=True)\n",
    "daily7 = Bui_Pw.values.reshape(int(Bui_Pw.shape[0] / 96), 96) \n",
    "dataPw = np.array([daily7],order='C')dataPw.shape \n",
    "dataPw.resize((339,96))\n",
    "DF5 = pd.DataFrame(dataPw) \n",
    "Bui_Pw_df = pd.DataFrame(data=dataPw, index=grp1.index, columns=DF.columns) \n",
    "Bui_Pw_df.columns = ['Pw' + str(i) for i in range(len(Bui_Pw_df.columns))] \n",
    "Bui_Pw_df.to_csv('BuiPw.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36862aa5",
   "metadata": {},
   "source": [
    "# Test _data (df_Targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54152b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targets = Bui_Pw_df df_targets.to_csv('df_targets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802a238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 0 # 0 or 11 \n",
    "k2 = 95 # 95 or 80 \n",
    "output_dim = k2 - k1 + 1\n",
    "model_name = 'GBR' \n",
    "n_estimators = 50 \n",
    "max_depth = 5 \n",
    "learning_rate = 1e-2 \n",
    "VS_days = 11 \n",
    "N_Q = 9 # Number of quantiles \n",
    "q_set = np.array([i / (N_Q+1) for i in range(1, N_Q + 1)]) # Set of quantiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that builds target shape for quantile forecasts\n",
    "if quantile: \n",
    "        target_list = [] \n",
    "        for col in df_targets.columns: \n",
    "            target_list += [df_targets[col]] * N_Q \n",
    " \n",
    "        shape = [nb_days, N_Q*(k2-k1+1)] \n",
    "        df_target_pv = pd.concat(target_list, axis=1, join='inner') \n",
    "        col_list = [] \n",
    "        for i in range(k1, k2 + 1): \n",
    "            col_list += ['q' + str(i) + '_' + str(j) for j in range(1, N_Q + 1)] \n",
    "        df_target_pv.columns = col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae0474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "df_LS_inputs, df_LS_targets = df_inputs[:328],df_inputs[328:] \n",
    "df_VS_inputs, df_VS_targets = df_targets[:328],df_targets[328:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff0f631",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LS_inputs.shape,df_LS_targets.shape,df_VS_inputs.shape,df_VS_targets.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6782e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bui_data_input \n",
    "Bui_data_input.index = pd.to_datetime(Bui_data_input['DATE'], format='%Y-%m-%d') \n",
    "Bui_data_input.drop('DATE',axis=1,inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee0c9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Buitest = Solar_data[['DATE','Wd','Pw','Pt','Ghi','At','Di','Ws']][31488:] \n",
    "Buitest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e676e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "Buitest_1 = Solar_data[['DATE','Wd','Pw','Pt','Ghi','At','Di','Ws']] \n",
    "Buitest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bb7bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Buitest_1.to_csv('Buitest_1.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5559c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Quantile_Pred.shape,Buitest.shape \n",
    "Bui_data_input \n",
    "Quantile_Pred = pd.read_csv(r'C:\\Users\\USER\\Downloads\\Bui Data\\QuantilePreds.csv') \n",
    "Quantile_Pred['DATE_C'] = pd.to_datetime(Quantile_Pred['DATE']).dt.date  \n",
    "Quantile_Pred.drop(['Unnamed: 0','DATE'],axis=1,inplace=True) \n",
    "Quantile_Pred['DATE']= Quantile_Pred['DATE_C'] # In[762]: \n",
    "Quantile_Pred.index= pd.to_datetime(Quantile_Pred['DATE'],format='%Y-%m-%d')\n",
    "Quantile_Pred.drop(['DATE_C','DATE'],axis=1,inplace=True) \n",
    "Quantile_Pred.columns= ['1','2','3','4','5','6','7','8','9']   \n",
    "df_targets_Pw = df_targets.copy() \n",
    "df_LS_targets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3d8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_q = 3 \n",
    "col_list = [] for i in range(k1, k2 + 1):\n",
    "    col_list += ['q' + str(i) + '_' + str(j) for j in range(1, N_Q + 1)] \n",
    "    col_list; \n",
    "Bui_Qn = Quantile_Pred.copy() \n",
    "dataQn = np.array([daily8],order='C') \n",
    "dataPw.shape \n",
    "dataPw.resize((339,96)) \n",
    " \n",
    "DF5 = pd.DataFrame(dataPw) \n",
    "Bui_Pw_df = pd.DataFrame(data=dataPw, index=grp1.index, columns=DF.columns) \n",
    "Bui_Pw_df.columns = ['Pw' + str(i) for i in range(len(Bui_Pw_df.columns))]  \n",
    "Bui_Pw_df.to_csv('BuiPw.csv') \n",
    "\n",
    "daily8 = Bui_Qn.values.reshape(int(Bui_Qn.shape[0] / 96),9*(k2-k1+1)) # In[400]: \n",
    "dataQn = np.array([daily8],order='C') # In[404]: #dataQn.shape # In[402]: \n",
    "dataQn.resize((11,864))  \n",
    "DF6 = pd.DataFrame(dataQn) \n",
    " \n",
    "# Bui_q1 \n",
    "Bui_Qn \n",
    "Bui_q1 = Bui_Qn[['1']] \n",
    "d1 = Bui_q1.values.reshape(int(Bui_q1.shape[0] / 96), 96) \n",
    "dataq1 = np.array([d1],order='C') dataq1.shape \n",
    "dataq1.resize((11,96))\n",
    "df1 = pd.DataFrame(dataq1) \n",
    "Bui_q1_df = pd.DataFrame(data=dataq1, index=df_LS_targets.index, columns=df1.columns) \n",
    "Bui_q1_df.columns = ['q' + str(i) for i in range(len(Bui_q1_df.columns))] \n",
    "Bui_q1_df.to_csv('Buiq1.csv') \n",
    "\n",
    "# # Bui_q2 \n",
    "Bui_q2 = Bui_Qn[['2']]  \n",
    "d2 = Bui_q2.values.reshape(int(Bui_q2.shape[0] / 96), 96) \n",
    "dataq2 = np.array([d2],order='C') dataq2.shape \n",
    "dataq2.resize((11,96)) \n",
    "df2 = pd.DataFrame(dataq2) \n",
    "Bui_q2_df = pd.DataFrame(data=dataq2, index=df_LS_targets.index, columns=df2.columns) \n",
    "Bui_q2_df.columns = ['q' + str(i) for i in range(len(Bui_q2_df.columns))] \n",
    "Bui_q2_df.to_csv('Buiq2.csv') \n",
    "Bui_q2_df \n",
    "\n",
    "# Bui_q3 \n",
    "Bui_q3 = Bui_Qn[['3']] \n",
    "d3 = Bui_q3.values.reshape(int(Bui_q3.shape[0] / 96), 96)\n",
    "dataq3 = np.array([d3],order='C') dataq3.shape\n",
    "dataq3.resize((11,96))  \n",
    "df3 = pd.DataFrame(dataq3) \n",
    "Bui_q3_df = pd.DataFrame(data=dataq3, index=df_LS_targets.index, columns=df3.columns) \n",
    "Bui_q3_df.columns = ['q' + str(i) for i in range(len(Bui_q3_df.columns))] \n",
    "Bui_q3_df.to_csv('Buiq3.csv') \n",
    "Bui_q3_d\n",
    "\n",
    "# Bui_q4 \n",
    "Bui_q4 = Bui_Qn[['4']] \n",
    "d4 = Bui_q4.values.reshape(int(Bui_q4.shape[0] / 96), 96)\n",
    "dataq4 = np.array([d4],order='C') dataq4.shape \n",
    "dataq4.resize((11,96)) \n",
    "df4 = pd.DataFrame(dataq4) \n",
    "Bui_q4_df = pd.DataFrame(data=dataq4, index=df_LS_targets.index, columns=df4.columns) \n",
    "Bui_q4_df.columns = ['q' + str(i) for i in range(len(Bui_q4_df.columns))] \n",
    "Bui_q4_df.to_csv('Buiq4.csv') \n",
    "Bui_q4_df \n",
    " \n",
    " # Bui_q5 \n",
    "Bui_q5 = Bui_Qn[['5']] \n",
    "d5 = Bui_q5.values.reshape(int(Bui_q5.shape[0] / 96), 96) \n",
    "dataq5 = np.array([d5],order='C') dataq5.shape \n",
    "dataq5.resize((11,96)) \n",
    "df5 = pd.DataFrame(dataq5) \n",
    "Bui_q5_df = pd.DataFrame(data=dataq5, index=df_LS_targets.index, columns=df5.columns) \n",
    "Bui_q5_df.columns = ['q' + str(i) for i in range(len(Bui_q5_df.columns))] \n",
    "Bui_q5_df.to_csv('Buiq5.csv') \n",
    "Bui_q5_df \n",
    " \n",
    "# Bui_q6\n",
    "Bui_q6 = Bui_Qn[['6']]  \n",
    "d6 = Bui_q6.values.reshape(int(Bui_q6.shape[0] / 96), 96) \n",
    "dataq6 = np.array([d6],order='C') dataq6.shape \n",
    "dataq6.resize((11,96)) \n",
    "df6 = pd.DataFrame(dataq6) \n",
    "Bui_q6_df = pd.DataFrame(data=dataq6, index=Bui_q5_df.index, columns=df6.columns) \n",
    "Bui_q6_df.columns = ['q' + str(i) for i in range(len(Bui_q6_df.columns))] \n",
    "Bui_q6_df.to_csv('Buiq6.csv') \n",
    " \n",
    "# Bui_q7  \n",
    "Bui_q7 = Bui_Qn[['7']] \n",
    "d7 = Bui_q7.values.reshape(int(Bui_q7.shape[0] / 96), 96)\n",
    "dataq7 = np.array([d7],order='C') dataq7.shape\n",
    "dataq7.resize((11,96)) \n",
    "df7 = pd.DataFrame(dataq7) \n",
    "Bui_q7_df = pd.DataFrame(data=dataq7, index=df_LS_targets.index, columns=df7.columns) \n",
    "Bui_q7_df.columns = ['q' + str(i) for i in range(len(Bui_q7_df.columns))] \n",
    "Bui_q7_df.to_csv('Buiq7.csv') \n",
    "\n",
    "# Bui_q8 \n",
    "Bui_q8 = Bui_Qn[['8']] \n",
    "d8 = Bui_q8.values.reshape(int(Bui_q7.shape[0] / 96), 96) \n",
    "dataq8 = np.array([d8],order='C') dataq8.shape \n",
    "dataq8.resize((11,96)) \n",
    "df8 = pd.DataFrame(dataq8) \n",
    "Bui_q8_df = pd.DataFrame(data=dataq8, index=df_LS_targets.index, columns=df8.columns) \n",
    "Bui_q8_df.columns = ['q' + str(i) for i in range(len(Bui_q8_df.columns))] \n",
    "Bui_q8_df.to_csv('Buiq8.csv') \n",
    " \n",
    "# Bui_q9  \n",
    "Bui_q9 = Bui_Qn[['9']] \n",
    "d9 = Bui_q9.values.reshape(int(Bui_q9.shape[0] / 96), 96) \n",
    "dataq9 = np.array([d9],order='C') dataq9.shape \n",
    "dataq9.resize((11,96)) df9 = pd.DataFrame(dataq9) \n",
    "Bui_q9_df = pd.DataFrame(data=dataq9, index=df_LS_targets.index, columns=df9.columns) \n",
    "Bui_q9_df.columns = ['q' + str(i) for i in range(len(Bui_q9_df.columns))] \n",
    "Bui_q9_df.to_csv('Buiq9.csv') \n",
    "\n",
    "# Add suffix to columns\n",
    "Bui_q2_df .columns = [str(col) + '_2' for col in Bui_q2_df .columns] \n",
    "Bui_q2_df.to_csv('Bui_q2.csv') \n",
    "Bui_q2_df \n",
    "\n",
    "# Add columns together \n",
    "df_predictions2 = pd.concat([Bui_q1_df,Bui_q2_df,Bui_q3_df,Bui_q4_df,Bui_q5_df,Bui_q6_df,Bui_q7_df,Bui_q8_df,Bui_q9_df],axis=1,join='inner') \n",
    "df_VS_targets.to_csv('df.csv') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99bc34e",
   "metadata": {},
   "source": [
    "# Persistence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f13ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "Solar_data1 = pd.read_csv(r'C:\\Users\\USER\\Desktop\\BUI PROJECT\\Bui_Data_P.csv') \n",
    "x_trainp, y_trainp = Solar_data1[['WIND DIRECTION','PANEL TEMPERATURE','GLOBAL IRRADIATION', 'AMBIENT TEMPERATURE','DIRECT IRRADIANCE','WIND SPEED']][:31488],Solar_data1.POWER[:31488] \n",
    "x_testp, y_testp = Solar_data1[['WIND DIRECTION','PANEL TEMPERATURE','GLOBAL IRRADIATION', \n",
    "                          'AMBIENT TEMPERATURE','DIRECT IRRADIANCE','WIND SPEED']][31488:],Solar_data1.POWER[31488:] \n",
    "pred_baseline = Solar_data1.POWER[30432:31488] \n",
    "pred_basper = pd.DataFrame(pred_baseline) #Baseline for persistence\n",
    "pred_baseline_N = Solar_data1[['DATE','POWER']][30432:31488]\n",
    "\n",
    "pred_baseline_N.index= pd.to_datetime(pred_baseline_N['DATE'],format='%m/%d/%Y') \n",
    "pred_baseline_N.drop('DATE',axis=1,inplace=True) \n",
    "\n",
    "p2 = pred_baseline_N.values.reshape(int(pred_baseline_N.shape[0] / 96), 96) \n",
    "datap2 = np.array([p2],order='C') datap2.shape \n",
    "datap2.resize((11,96)) \n",
    " \n",
    "persistence2 = pd.DataFrame(datap2,index=df_VS_targets.index) \n",
    "pred_basper.reset_index(inplace=True)  \n",
    "pred_basper.drop('index',axis=1,inplace=True)  \n",
    "pred_basper  \n",
    "p1 = pred_basper.values.reshape(int(pred_basper.shape[0] / 96), 96) \n",
    "datap = np.array([p1],order='C') \n",
    "datap.shape \n",
    "datap.resize((11,96)) \n",
    "persistence = pd.DataFrame(datap,index=df_VS_targets.index) persistence.reset_index() \n",
    " \n",
    "\n",
    "persistence.reset_index(inplace=True)\n",
    "persistence.drop(['index','DATE'],axis=1,inplace=True) \n",
    "df_VS_targets \n",
    "\n",
    "def point_scores(y_true:np.array, y_pred:np.array, k1:int, k2:int): \n",
    "    \"\"\" \n",
    "    Compute NMAE and NRMSE. \n",
    "    --------- \n",
    "    y_true : array-like of shape (n_samples,) or (n_samples, n_outputs) -> n_outputs = nb of forecasting periods. \n",
    "    Ground truth (correct) target values. \n",
    " \n",
    "    y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs) -> n_outputs = nb of forecasting periods. \n",
    "    Estimated target values. \n",
    " \n",
    "    k1 : first forecasting period. -> 0 <= k1 <= 95     k2 : last forecasting period. -> 0 <= k2 <= 95 and k1 <= k2. \n",
    " \n",
    "    Returns \n",
    "    ------- \n",
    "    pd.DataFrame with the NMAE and NRMSE per forecasting time period. \n",
    "    \"\"\" \n",
    "    nmae = 100 * mean_absolute_error(y_true=y_true, y_pred=y_pred, multioutput='raw_values')/ 50 \n",
    "    nrmse = 100 * np.sqrt(mean_squared_error(y_true=y_true, y_pred=y_pred, multioutput='raw_values'))  / 50    \n",
    "    df_scores = pd.DataFrame(data=nmae, columns=['NMAE']) \n",
    "    df_scores['NRMSE'] = nrmse \n",
    "    df_scores.index = [i for i in range(k1, k2 + 1)]\n",
    "    \n",
    "return df_scores \n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------ \n",
    "# COMPUTE NMAE and NRMSE \n",
    "# ------------------------------------------------------------------------------------------------------------------ \n",
    "df_scores = point_scores(y_true=df_VS_targets.values, y_pred=persistence2.values, k1=k1, k2=k2) print(df_scores.mean()) \n",
    " \n",
    "dirname = r'C:\\Users\\USER\\Desktop\\Bui Masters Project\\PV data\\Persistence\\New folder' \n",
    "if not os.path.isdir(dirname):  # test if directory exist     \n",
    "    os.makedirs(dirname) df_scores.to_csv(dirname + 'point_scores_' + model_name + '_' + str(k1) + '_' + str(k2) + '.csv') \n",
    " \n",
    "plot_point_metrics(df_scores=df_scores, dir=dirname, model_name=model_name, k1=k1, k2=k2) \n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------ \n",
    "# COMPUTE NMAE and NRMSE \n",
    "# ------------------------------------------------------------------------------------------------------------------ \n",
    "df_scores = point_scores(y_true=df_VS_targets.values, y_pred=Pop.values, k1=k1, k2=k2) print(df_scores.mean()) \n",
    " \n",
    "dirname = r'C:\\Users\\USER\\Desktop\\Bui Masters Project\\PV data\\Persistence' if not os.path.isdir(dirname):  # test if directory exist    \n",
    "    os.makedirs(dirname) df_scores.to_csv(dirname + 'point_scores_' + model_name + '_' + str(k1) + '_' + str(k2) + '.csv') \n",
    " \n",
    "plot_point_metrics(df_scores=df_scores, dir=dirname, model_name=model_name, k1=k1, k2=k2) \n",
    "\n",
    "def plot_point_forecasts(df_predictions:Pop, df_target:df_VS_targets, dir:str, model_name:str, k1: int=0, k2: int=95): \n",
    " \n",
    "    x_index = [i for i in range(k1, k2+1)]   \n",
    "    for day in df_predictions.index:     \n",
    "        FONTSIZE = 20 \n",
    "        plt.figure()         \n",
    "        plt.plot(x_index, df_target.loc[day].values, 'r', linewidth=1, label='Actual') \n",
    "        plt.plot(x_index, df_predictions.loc[day].values, 'k', linewidth=1, label='Preds')\n",
    "        plt.ylim(0, 50)         \\\n",
    "        plt.ylabel('MW', fontsize=15, rotation='horizontal')        \n",
    "        plt.xticks(fontsize=FONTSIZE)        \n",
    "        plt.yticks(fontsize=FONTSIZE)        \n",
    "        plt.legend(fontsize=15)        \n",
    "        plt.tight_layout() \n",
    "        plt.savefig(dir + 'point_' + day.strftime('%Y%m%d') + '_' + model_name + '_' + str(k1) + '_' + str(k2) + '.pdf')         \n",
    "        plt.close('all')  \n",
    "# ------------------------------------------------------------------------------------------------------------------ \n",
    "# PLOTS \n",
    "# ------------------------------------------------------------------------------------------------------------------ \n",
    "# Create folder \\\n",
    "dirname = r'C:\\Users\\USER\\Desktop\\Bui Masters Project\\PV data\\Persistence' \n",
    "if not os.path.isdir(dirname):  # test if directory exist     \n",
    "    os.makedirs(dirname) \n",
    " \n",
    "plot_point_forecasts(df_predictions=Pop, df_target=df_VS_targets, dir=dirname, model_name=model_name, k1=k1, k2=k2) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a300d4c5",
   "metadata": {},
   "source": [
    "# Case 1: Random Forest – deterministic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb5c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regr = RandomForestRegressor() \n",
    "regr.fit(df_LS_inputs_R,df_LS_targets_R) \n",
    "predregr = regr.predict(df_VS_inputs_R) \n",
    "r2_score(predregr,df_VS_targets_R) \n",
    "mean_absolute_error(predregr,df_VS_targets_R)\n",
    "predregr \n",
    "predreg = pd.DataFrame(predregr,index=Bui_Qn.index) predreg \n",
    "d12 = predreg.values.reshape(int(predreg.shape[0] / 96), 96) \n",
    "dataq12 = np.array([d12],order='C') dataq12.shape \n",
    "dataq12.resize((11,96)) \n",
    "df12 = pd.DataFrame(dataq12,index=df_VS_targets.index)\n",
    "predictreg=df12 \n",
    "model_name1 = 'Randomforest' \n",
    "# Create folder \n",
    "dirname = r'C:\\Users\\USER\\Desktop\\Bui Masters Project\\PV data\\RandomPoint\\New folder'\n",
    "if not os.path.isdir(dirname):  # test if directory exist     \n",
    "    os.makedirs(dirname) predictreg.to_csv(dirname + 'dad_point_' + model_name1 + '_' + str(k1) + '_' + str(k2) + '.csv')  \n",
    "# ------------------------------------------------------------------------------------------------------------------ \n",
    "# COMPUTE NMAE and NRMSE \n",
    "# ------------------------------------------------------------------------------------------------------------------ \n",
    "df_scores = point_scores(y_true=df_VS_targets.values, y_pred=predictreg.values, k1=k1, k2=k2) print(df_scores.mean()) \n",
    " \n",
    "dirname = r'C:\\Users\\USER\\Desktop\\Bui Masters Project\\PV data\\RandomPoint\\New folder' \n",
    "if not os.path.isdir(dirname):  # test if directory exist    \n",
    "    os.makedirs(dirname) \n",
    "df_scores.to_csv(dirname + 'point_scores_' + model_name1 + '_' + str(k1) + '_' + str(k2) + '.csv') \n",
    "plot_point_metrics(df_scores=df_scores, dir=dirname, model_name=model_name1, k1=k1, k2=k2) \n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------ \n",
    "# PLOTS \n",
    "# ------------------------------------------------------------------------------------------------------------------ \n",
    "# Create folder \n",
    "dirname = r'C:\\Users\\USER\\Desktop\\Bui Masters Project\\PV data\\RandomPoint\\New folder' \n",
    "if not os.path.isdir(dirname):  # test if directory exist     \n",
    "    os.makedirs(dirname) \n",
    "plot_point_forecasts(df_predictions=predictreg, df_target=df_VS_targets, dir=dirname, model_name=model_name1, k1=k1, k2=k2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653affaf",
   "metadata": {},
   "source": [
    "# Case 2: Gradient Boosting – deterministic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07306c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "point = pd.read_csv(r'C:\\Users\\USER\\Downloads\\Buitest_1.csv')  \n",
    "point.drop(['Unnamed: 0','DATE'],axis=1,inplace=True) \n",
    "\n",
    "#train test split \n",
    "df_LS_inputs_R, df_LS_targets_R = point[['Wd','Pt','Ghi','At','Di','Ws']][:31488],point.Pw[:31488] \n",
    "df_VS_inputs_R, df_VS_targets_R = point[['Wd','Pt','Ghi','At','Di','Ws']][31488:],point.Pw[31488:] \n",
    "df_LS_inputs_R.shape, df_LS_targets_R.shape,df_VS_inputs_R.shape, df_VS_targets_R.shape\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score \n",
    "\n",
    "gradientregressor = GradientBoostingRegressor(max_depth=5,n_estimators=500)\n",
    "GBRmodel_P = gradientregressor.fit(df_LS_inputs_R,df_LS_targets_R) \n",
    "pred_P = GBRmodel_P.predict(df_VS_inputs_R)\n",
    "r2_score(pred_P, df_VS_targets_R) \n",
    "mean_absolute_error(pred_P, df_VS_targets_R) \n",
    " \n",
    "df_predictions = pd.DataFrame(data=pred_P, index=df_VS_targets.index, columns=df_VS_targets.columns).sort_index() \n",
    "df_VS_targets.columns \n",
    "pred_P \n",
    "Bui_Pw_df \n",
    "x = Bui_Pw_df[:328] \n",
    "y = Bui_Pw_df[328:] \n",
    "new_prez = pd.DataFrame(pred_P,index=Bui_Qn.index) \n",
    "d11 = new_prez.values.reshape(int(new_prez.shape[0] / 96), 96) \n",
    "dataq11 = np.array([d11],order='C') dataq11.shape \n",
    "dataq11.resize((11,96))  \n",
    "df11 = pd.DataFrame(dataq11,index=df_VS_targets.index) \n",
    "predictions=df11\n",
    "\n",
    "\n",
    "# Create folder\n",
    "dirname = r'C:\\Users\\USER\\Desktop\\Bui Masters Project\\PV data\\GBRPoint\\New folder' \n",
    "if not os.path.isdir(dirname):  # test if directory exist     \n",
    "    os.makedirs(dirname) predictions.to_csv(dirname + 'dad_point_' + model_name + '_' + str(k1) + '_' + str(k2) + '.csv') \n",
    "    \n",
    "def point_scores(y_true:np.array, y_pred:np.array, k1:int, k2:int): \n",
    "    \"\"\" \n",
    "    Compute NMAE and NRMSE. \n",
    "    --------- \n",
    "    y_true : array-like of shape (n_samples,) or (n_samples, n_outputs) -> n_outputs = nb of forecasting periods. \n",
    "    Ground truth (correct) target values.     y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs) -> n_outputs = nb of forecasting periods.     \n",
    "    Estimated target values. \n",
    "    k1 : first forecasting period. -> 0 <= k1 <= 95     k2 : last forecasting period. -> 0 <= k2 <= 95 and k1 <= k2. \n",
    "   Returns \n",
    "    ------- \n",
    "    pd.DataFrame with the NMAE and NRMSE per forecasting time period. \n",
    "    \"\"\" \n",
    " \n",
    "    nmae = 100 * mean_absolute_error(y_true=y_true, y_pred=y_pred, multioutput='raw_values') / 50 \n",
    "    nrmse = 100 * np.sqrt(mean_squared_error(y_true=y_true, y_pred=y_pred, multioutput='raw_values'))  / 50     \n",
    "    df_scores = pd.DataFrame(data=nmae, columns=['NMAE'])     \n",
    "    df_scores['NRMSE'] = nrmse \n",
    "    df_scores.index = [i for i in range(k1, k2 + 1)] \n",
    "return df_scores \n",
    "\n",
    "\n",
    "def plot_point_forecasts(df_predictions:pd.DataFrame, df_target:pd.DataFrame, dir:str, model_name:str, k1: int=0, k2: int=95): \n",
    " \n",
    "    x_index = [i for i in range(k1, k2+1)] \n",
    "    for day in df_predictions.index:         \n",
    "        FONTSIZE = 20 \n",
    "        plt.figure()        \n",
    "        plt.plot(x_index, df_target.loc[day].values, 'r', linewidth=1, label='Actual') \n",
    "        plt.plot(x_index, df_predictions.loc[day].values, 'b', linewidth=1, label='Preds')      \n",
    "        plt.ylim(0, 50)      \n",
    "        plt.ylabel('MW', fontsize=15, rotation='horizontal')    \n",
    "        plt.xticks(fontsize=FONTSIZE)      \n",
    "        plt.yticks(fontsize=FONTSIZE)     \n",
    "        plt.legend(fontsize=15)      \n",
    "        plt.tight_layout() \n",
    "        plt.savefig(dir + 'point_' + day.strftime('%Y%m%d') + '_' + model_name + '_' + str(k1) + '_' + str(k2) + '.pdf')         \n",
    "        plt.close('all') \n",
    "        \n",
    "def plot_point_metrics(df_scores: pd.DataFrame, dir: str, model_name: str, k1: int = 0, k2: int = 95): \n",
    "        FONTSIZE = 20 \n",
    "        plt.figure()     \n",
    "        plt.plot(df_scores.index, df_scores['NMAE'].values, linewidth=1, color='k', label='NMAE')   \n",
    "        plt.plot(df_scores.index, df_scores['NRMSE'].values, linewidth=1, color='r', label='NRMSE') \n",
    "        plt.ylim(0, 25)  \n",
    "        plt.ylabel('%', fontsize=FONTSIZE, rotation='horizontal')    \n",
    "        plt.xticks(fontsize=FONTSIZE)    \n",
    "        plt.yticks(fontsize=FONTSIZE)     \n",
    "        plt.legend(fontsize=15)     \n",
    "        plt.tight_layout()    \n",
    "        plt.savefig(dir + 'point_scores_' + model_name + '_' + str(k1) + '_' + str(k2) + '.pdf') \n",
    "        plt.close('all') \n",
    " \n",
    " if __name__ == \"__main__\": \n",
    "        print('ok') \n",
    "        \n",
    "# ------------------------------------------------------------------------------------------------------------------ \n",
    "# COMPUTE NMAE and NRMSE \n",
    "# ------------------------------------------------------------------------------------------------------------------ \n",
    "df_scores = point_scores(y_true=df_VS_targets.values, y_pred=predictions.values, k1=k1, k2=k2) print(df_scores.mean()) \n",
    " \n",
    "dirname = r'C:\\Users\\USER\\Desktop\\Bui Masters Project\\PV data\\GBRPoint\\New folder' \n",
    "if not os.path.isdir(dirname):  # test if directory exist     \n",
    "    os.makedirs(dirname) \n",
    "    df_scores.to_csv(dirname + 'point_scores_' + model_name + '_' + str(k1) + '_' + str(k2) + '.csv') \n",
    " \n",
    "plot_point_metrics(df_scores=df_scores, dir=dirname, model_name=model_name, k1=k1, k2=k2) \n",
    "# ------------------------------------------------------------------------------------------------------------------ \n",
    "# PLOTS \n",
    "# ------------------------------------------------------------------------------------------------------------------ \n",
    "# Create folder \n",
    "dirname = r'C:\\Users\\USER\\Desktop\\Bui Masters Project\\PV data\\GBRPoint\\New folder' \n",
    "if not os.path.isdir(dirname):  # test if directory exist     \n",
    "    os.makedirs(dirname) \n",
    " \n",
    "plot_point_forecasts(df_predictions=predictions, df_target=df_VS_targets, dir=dirname, model_name=model_name, k1=k1, k2=k2) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af90e41e",
   "metadata": {},
   "source": [
    "# Case 3: Random Forest – Probabilistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7b3009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib as mpl \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import statsmodels.api as sm\n",
    "from sklearn import ensemble \n",
    "\n",
    " \n",
    "METHODS = ['quantreg',] \n",
    "METHODS_PRINT = ['QuantReg'] \n",
    "QUANTILES = [0.1,0.2,0.3,0.4, 0.5,0.6,0.7,0.8, 0.9] \n",
    "quantiles_legend = ['10th percentile', '50th percentile', '90th percentile'] \n",
    "Solar_data = pd.read_csv(r'C:\\Users\\USER\\Desktop\\BUI PROJECT\\Bui_Data_P.csv') \n",
    "Solar_data1 = Solar_data[['DATE','TIME','WIND DIRECTION','PANEL TEMPERATURE','GLOBAL IRRADIATION','AMBIENT TEMPERATURE','DIRECT IRRADIANCE','WIND SPEED','POWER']]  \n",
    "\n",
    "#train test split \n",
    "x_train, y_train = Solar_data1[['WIND DIRECTION','PANEL TEMPERATURE','GLOBAL IRRADIATION', 'AMBIENT TEMPERATURE',\n",
    "                                'DIRECT IRRADIANCE','WIND SPEED']][:31488],Solar_data1.POWER[:31488] \n",
    "x_test, y_test =  Solar_data1[['WIND DIRECTION','PANEL TEMPERATURE','GLOBAL IRRADIATION', 'AMBIENT TEMPERATURE',\n",
    "                               'DIRECT IRRADIANCE','WIND SPEED']][31488:],Solar_data1.POWER[31488:] \n",
    "\n",
    "x_test_ghi = x_test[['DIRECT IRRADIANCE']]  \n",
    "preds = np.array([(method, q, x)for method in METHODS for q in QUANTILES for x in x_test_ghi.values]) \n",
    "preds = pd.DataFrame(preds) preds.columns = ['method', 'q', 'x'] \n",
    "preds = preds.apply(lambda x: pd.to_numeric(x, errors='ignore')) \n",
    "preds['x'] = preds['x'].str.get(0) \n",
    "\n",
    "from sklearn.ensemble\n",
    "import RandomForestRegressor \n",
    "reg = RandomForestRegressor() \n",
    "reg.fit(x_train,y_train) \n",
    "prediction = reg.predict(x_test) \n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score r2_score(prediction,y_test) \n",
    "\n",
    "def rf_quantile(m, X, q): \n",
    "    rf_preds = []     \n",
    "    for estimator in m.estimators_: \n",
    "        rf_preds.append(estimator.predict(X))     \n",
    "        rf_preds = np.array(rf_preds).transpose()  # One row per record. \n",
    "return np.percentile(rf_preds, q * 100, axis=1) \n",
    " \n",
    "preds.loc[preds.method == 'rf', 'pred'] = np.concatenate([rf_quantile(reg, x_test, q) for q in QUANTILES])  \n",
    "preds.to_csv('Bui_rf2.csv')\n",
    "\n",
    "regQR = pd.read_csv(r'C:\\Users\\USER\\Downloads\\Bookrfq.csv') \n",
    " \n",
    "date_in = pd.read_csv(r'C:\\Users\\USER\\Downloads\\Bui Data\\Bui_q\\Bui_q8.csv') \n",
    "date_in.index = pd.to_datetime(date_in['DATE'],format='%Y-%m-%d')  \n",
    "Bui_q1 = regQR[['1']] d1 = Bui_q1.values.reshape(int(Bui_q1.shape[0] / 96), 96) \n",
    "dataq1 = np.array([d1],order='C') \n",
    "dataq1.shape dataq1.resize((11,96)) \n",
    " \n",
    "df1 = pd.DataFrame(dataq1) \n",
    "Bui_q1_df = pd.DataFrame(data=dataq1, index=date_in.index, columns=df1.columns) \n",
    "Bui_q1_df.columns = ['q' + str(i) for i in range(len(Bui_q1_df.columns))] \n",
    "Bui_q1_df \n",
    " \n",
    "Bui_q2 = regQR[['2']] \n",
    " \n",
    "d2 = Bui_q2.values.reshape(int(Bui_q2.shape[0] / 96), 96) \n",
    "dataq2 = np.array([d2],order='C') dataq2.shape\n",
    "dataq2.resize((11,96)) \n",
    " \n",
    "df2 = pd.DataFrame(dataq2) \n",
    "Bui_q2_df = pd.DataFrame(data=dataq2, index=date_in.index, columns=df2.columns) \n",
    "Bui_q2_df.columns = ['q' + str(i) for i in range(len(Bui_q2_df.columns))] \n",
    " \n",
    "Bui_q2_df \n",
    "Bui_q3 = regQR[['3']] d3 = Bui_q3.values.reshape(int(Bui_q3.shape[0] / 96), 96) \n",
    "dataq3 = np.array([d3],order='C') dataq3.shape \n",
    "dataq3.resize((11,96)) \n",
    " \n",
    "df3 = pd.DataFrame(dataq3) \n",
    "Bui_q3_df = pd.DataFrame(data=dataq3, index=date_in.index, columns=df3.columns) \n",
    "Bui_q3_df.columns = ['q' + str(i) for i in range(len(Bui_q3_df.columns))] \n",
    " \n",
    "Bui_q3_df.to_csv('Buiq3.csv') \n",
    "Bui_q3_df \n",
    "Bui_q4 = regQR[['4']] \n",
    " \n",
    "d4 = Bui_q4.values.reshape(int(Bui_q4.shape[0] / 96), 96) \n",
    "dataq4 = np.array([d4],order='C') dataq4.shape \n",
    "dataq4.resize((11,96)) \n",
    " \n",
    "df4 = pd.DataFrame(dataq4) \n",
    " \n",
    "Bui_q4_df = pd.DataFrame(data=dataq4, index=date_in.index, columns=df4.columns) \n",
    "Bui_q4_df.columns = ['q' + str(i) for i in range(len(Bui_q4_df.columns))] \n",
    " \n",
    "Bui_q4_df.to_csv('Buiq4.csv') \n",
    "Bui_q4_df \n",
    "Bui_q5 = regQR[['5']] \n",
    " \n",
    "d5 = Bui_q5.values.reshape(int(Bui_q5.shape[0] / 96), 96)\n",
    "dataq5 = np.array([d5],order='C') dataq5.shape \n",
    "dataq5.resize((11,96)) \n",
    " \n",
    "df5 = pd.DataFrame(dataq5) \n",
    "Bui_q5_df = pd.DataFrame(data=dataq5, index=date_in.index, columns=df5.columns) \n",
    "Bui_q5_df.columns = ['q' + str(i) for i in range(len(Bui_q5_df.columns))] \n",
    " \n",
    "Bui_q5_df.to_csv('Buiq5.csv') \n",
    "Bui_q5_df \n",
    "Bui_q6 = regQR[['6']] \n",
    " \n",
    "d6 = Bui_q6.values.reshape(int(Bui_q6.shape[0] / 96), 96) \n",
    "dataq6 = np.array([d6],order='C') dataq6.shape \n",
    "dataq6.resize((11,96)) \n",
    " \n",
    "df6 = pd.DataFrame(dataq6) \n",
    "Bui_q6_df = pd.DataFrame(data=dataq6, index=date_in.index, columns=df6.columns) \n",
    "Bui_q6_df.columns = ['q' + str(i) for i in range(len(Bui_q6_df.columns))] \n",
    " \n",
    "Bui_q6_df.to_csv('Buiq6.csv') \n",
    "Bui_q6_df \n",
    "  \n",
    "Bui_q7 = regQR[['7']] \n",
    " \n",
    "d7 = Bui_q7.values.reshape(int(Bui_q7.shape[0] / 96), 96) \n",
    "dataq7 = np.array([d7],order='C') dataq7.shape \n",
    "dataq7.resize((11,96)) \n",
    " \n",
    "df7 = pd.DataFrame(dataq7) \n",
    "Bui_q7_df = pd.DataFrame(data=dataq7, index=date_in.index, columns=df7.columns) \n",
    "Bui_q7_df.columns = ['q' + str(i) for i in range(len(Bui_q7_df.columns))] \n",
    " \n",
    "Bui_q7_df.to_csv('Buiq7.csv') \n",
    "Bui_q7_df \n",
    " \n",
    "Bui_q8 = regQR[['8']] \n",
    " \n",
    "d8 = Bui_q8.values.reshape(int(Bui_q7.shape[0] / 96), 96) \n",
    "dataq8 = np.array([d8],order='C') dataq8.shape \n",
    "dataq8.resize((11,96)) \n",
    " \n",
    "df8 = pd.DataFrame(dataq8) \n",
    "Bui_q8_df = pd.DataFrame(data=dataq8, index=date_in.index, columns=df8.columns) \n",
    "Bui_q8_df.columns = ['q' + str(i) for i in range(len(Bui_q8_df.columns))] \n",
    " \n",
    "Bui_q8_df.to_csv('Buiq8.csv') \n",
    "Bui_q8_df \n",
    "  \n",
    "Bui_q9 = regQR[['9']] \n",
    " \n",
    "d9 = Bui_q9.values.reshape(int(Bui_q9.shape[0] / 96), 96) \n",
    "dataq9 = np.array([d9],order='C') dataq9.shape \n",
    "dataq9.resize((11,96)) \n",
    " \n",
    "df9 = pd.DataFrame(dataq9) \n",
    "Bui_q9_df = pd.DataFrame(data=dataq9, index=date_in.index, columns=df9.columns) \n",
    "Bui_q9_df.columns = ['q' + str(i) for i in range(len(Bui_q9_df.columns))] \n",
    " \n",
    "Bui_q9_df.to_csv('Buiq9.csv') \n",
    "Bui_q9_df  \n",
    "Bui_q1_df .columns = [str(col) + '_1' for col in Bui_q1_df .columns] \n",
    "Bui_q2_df .columns = [str(col) + '_2' for col in Bui_q2_df .columns] \n",
    "Bui_q3_df .columns = [str(col) + '_3' for col in Bui_q3_df .columns] \n",
    "Bui_q4_df .columns = [str(col) + '_4' for col in Bui_q4_df .columns] \n",
    "Bui_q5_df .columns = [str(col) + '_5' for col in Bui_q5_df .columns] \n",
    "Bui_q6_df .columns = [str(col) + '_6' for col in Bui_q6_df .columns] \n",
    "Bui_q7_df .columns = [str(col) + '_7' for col in Bui_q7_df .columns] \n",
    "Bui_q8_df .columns = [str(col) + '_8' for col in Bui_q8_df .columns] \n",
    "Bui_q9_df .columns = [str(col) + '_9' for col in Bui_q9_df .columns] \n",
    " \n",
    "\n",
    "df_predictions = pd.concat([Bui_q1_df,Bui_q2_df,Bui_q3_df,Bui_q4_df,Bui_q5_df,Bui_q6_df,Bui_q7_df,Bui_q 8_df,Bui_q9_df],axis=1,join='inner')\n",
    "df_predictions\n",
    "df_predictions = df_predictions[['q0_1','q0_2','q0_3','q0_4','q0_5','q0_6','q0_7','q0_8','q0_9','q1_1','q1_2','q1_3','q1 _4','q1_5','q1_6','q1_7','q1_8','q1_9','q2_1','q2_2','q2_3','q2_4','q2_5','q2_6','q2_7','q2_8','q2_9','q 3_1','q3_2','q3_3','q3_4','q3_5','q3_6','q3_7','q3_8','q3_9','q4_1','q4_2','q4_3','q4_4','q4_5','q4_6',' q4_7','q4_8','q4_9','q5_1','q5_2','q5_3','q5_4','q5_5','q5_6','q5_7','q5_8','q5_9','q6_1','q6_2','q6_3', 'q6_4','q6_5','q6_6','q6_7','q6_8','q6_9','q7_1','q7_2','q7_3','q7_4','q7_5','q7_6','q7_7','q7_8','q7_9'\n",
    ",'q8_1','q8_2','q8_3','q8_4','q8_5','q8_6','q8_7','q8_8','q8_9','q9_1','q9_2','q9_3','q9_4','q9_5','q9_6','q9_7','q9_8','q9_9','q10_1','q10_2','q10_3','q10_4','q10_5','q10_6','q10_7','q10_8','q10_9','q11_1','q11_2','q11_3','q11_4','q11_5','q11_6','q11_7','q11_8','q11_9','q12_1','q12_2','q12_3','q12_4','q12_5','q12_6','q12_7','q12_8','q12_9','q13_1','q13_2','q13_3','q13_4','q13_5','q13_6','q13_7','q13_8','q13_9','q14_1','q14_2','q14_3','q14_4','q14_5','q14_6','q14_7','q14_8','q14_9','q15_1','q15_2','q15_3','q15_4','q15_5','q15_6','q15_7',\n",
    "'q15_8','q15_9','q16_1','q16_2','q16_3','q16_4','q16_5','q16_6','q16_7','q16_8','q16_9','q17_1','q17_2','q17_3','q17_4','q17_5','q17_6','q17_7','q17_8','q17_9','q18_1','q18_2','q18_3','q18_4','q18_5','q18_6','q18_7','q18_8','q18_9','q19_1','q19_2','q19_3','q19_4','q19_5','q19_6','q19_7','q19_8','q19_9','q20_1','q20_2','q20_3','q20_4','q20_5','q20_6','q20_7','q20_8','q20_9','q21_1','q21_2','q21_3','q21_4','q21_5','q21_6','q21_7','q21_8','q21_9','q22_1','q22_2','q22_3','q22_4','q22_5','q22_6','q22_7','q22_8','q22_9','q23_1','q23_2','q23_3',\n",
    "'q23_4','q23_5','q23_6','q23_7','q23_8','q23_9','q24_1','q24_2','q24_3','q24_4','q24_5','q24_6','q24_7','q24_8','q24_9','q25_1','q25_2','q25_3','q25_4','q25_5','q25_6','q25_7','q25_8','q25_9','q26_1','q26_2','q26_3','q26_4','q26_5','q26_6','q26_7','q26_8','q26_9','q27_1','q27_2','q27_3','q27_4','q27_5','q27_6','q27_','q27_8','q27_9','q28_1','q28_2','q28_3','q28_4','q28_5','q28_6','q28_7','q28_8','q28_9','q29_1','q29_2','q29_3','q29_4','q29_5','q29_6','q29_7','q29_8','q29_9','q30_1','q30_2','q30_3','q30_4',\n",
    "'q30_5','q30_6','q30_7','q30_8','q30_9','q31_1','q31_2','q31_3','q31_4','q31_5','q31_6','q31_7','q31_8','q31_9','q32_1','q32_2','q32_3','q32_4','q32_5','q32_6','q32_7','q32_8','q32_9','q33_1','q33_2','q33_','q33_4','q33_5','q33_6','q33_7','q33_8','q33_9','q34_1','q34_2','q34_3','q34_4','q34_5','q34_6','q34_7','q34_8','q34_9','q35_1','q35_2','q35_3','q35_4','q35_5','q35_6','q35_7','q35_8','q35_9','q36_1','q36_2','q36_3','q36_4','q36_5','q36_6','q36_7','q36_8','q36_9','q37_1','q37_2','q37_3','q37_4','q3 7_5',\n",
    "'q37_6','q37_7','q37_8','q37_9','q38_1','q38_2','q38_3','q38_4','q38_5','q38_6','q38_7','q38_8','q38_9','q39_1','q39_2','q39_3','q39_4','q39_5','q39_6','q39_7','q39_8','q39_9','q40_1','q40_2','q40_3','q40_4','q40_5','q40_6','q40_7','q40_8','q40_9','q41_1','q41_2','q41_3','q41_4','q41_5','q41_6','q41_7','q41_8','q41_9','q42_1','q42_2','q42_3','q42_4','q42_5','q42_6','q42_7','q42_8','q42_9','q3_1','q43_2','q43_3','q43_4','q43_5','q43_6','q43_7','q43_8','q43_9','q44_1','q44_2','q44_3','q44_4\n",
    "'q58_4','q58_5','q58_6','q58_7','q58_8','q58_9','q59_1','q59_2','q59_3','q59_4','q59_5','q59_6','q5','q44_5','q44_6','q44_7','q44_8','q44_9','q45_1','q45_2','q45_3','q45_4','q45_5','q45_6','q45_7','q45_8','q45_9','q46_1','q46_2','q46_3','q46_4','q46_5','q46_6','q46_7','q46_8','q46_9','q47_1','q47_2','q47_3','q47_4','q47_5','q47_6','q47_7','q47_8','q47_9','q48_1','q48_2','q48_3','q48_4','q48_5','q48_6','q48_7','q48_8','q48_9','q49_1','q49_2','q49_3','q49_4','q49_5','q49_6','q49_7','q49_8','q49_9','q50_1',\n",
    "'q50_2','q50_3','q50_4','q50_5','q50_6','q50_7','q50_8','q50_9','q51_1','q51_2','q51_3','q51_4','q51_5','q51_6','q51_7','q51_8','q51_9','q52_1','q52_2','q52_3','q52_4','q52_5','q52_6','q52_7','q52_8','q52_9','q53_1','q53_2','q53_3','q53_4','q53_5','q53_6','q53_7','q53_8','q53_9','q54_1','q54_2','q54_3','q54_4','q54_5','q54_6','q54_7','q54_8','q54_9','q55_1','q55_2','q55_3','q55_4','q55_5','q55_6','q55_7','q55_8','q55_9','q56_1','q56_2','q56_3','q56_4','q56_5','q56_6','q56_7','q56_8','q56_9','q57_1','q57_2',\n",
    " 'q57_3','q57_4','q57_5','q57_6','q57_7','q57_8','q57_9','q58_1','q58_2','q58_3',\"9_7','q59_8','q59_9','q60_1','q60_2','q60_3','q60_4','q60_5','q60_6','q60_7','q60_8','q60_9','q61_','q61_2','q61_3','q61_4','q61_5','q61_6','q61_7','q61_8','q61_9','q62_1','q62_2','q62_3','q62_4','q62_5','q62_6','q62_7','q62_8','q62_9','q63_1','q63_2','q63_3','q63_4','q63_5','q63_6','q63_7','q63_8','q63_9','q64_1','q64_2','q64_3','q64_4','q64_5','q64_6','q64_7','q64_8','q64_9','q65_1','q65_2','q65_3','q65_4','q65_5','q65_6','q65_7','q65_8','q65_9','q66_1',\n",
    "'q66_2','q66_3','q66_4','q66_5','q66_6','q66_7','q66_8','q66_9','q67_1','q67_2','q67_3','q67_4','q67_5','q67_6','q67_7','q67_8','q67_9','q8_1','q68_2','q68_3','q68_4','q68_5','q68_6','q68_7','q68_8','q68_9','q69_1','q69_2','q69_3','q69_4','q69_5','q69_6','q69_7','q69_8','q69_9','q70_1','q70_2','q70_3','q70_4','q70_5','q70_6','q70_7','q70_8','q70_9','q71_1','q71_2','q71_3','q71_4','q71_5','q71_6','q71_7','q71_8','q71_9','q72_1','q72_2','q72_3','q72_4','q72_5','q72_6','q72_7','q72_8','q72_9','q73_1','q73_2',\n",
    "'q73_3','q73_4','q73_5','q73_6','q73_7','q73_8','q73_9','q74_1','q74_2','q74_3','q74_4','q74_5','q74_6','q74_7','q74_8','q74_9','q75_1','q75_2','q75_3','q75_4','q75_5','q75_6','q75_7','q75_8','q75_9','q76_1','q76_2','q76_3','q76_4','q76_5','q76_6','q76_7','q76_8','q76_9','q77_1','q77_2','q77_3','q77_4','q77_5','q77_6','q77_7','q77_8','q77_9','q78_1','q78_2','q78_3','q78_4','q78_5','q78_6','q78_7','q78_8','q78_9','q79_1','q79_2','q79_3','q79_4','q79_5','q79_6','q79_7','q79_8','q79_9','q80_1','q80_2','q80_3','q80_4','q80_5','q80_6','q80_7',\n",
    "'q80_8','q80_9','q81_1','q81_2','q81_3','q81_4','q81_5','q81_6','q81_7','q81_8','q81_9','q82_1','q82_2','q82_3','q82_4','q82_5','q82_6','q82_7','q82_8','q82_9','q83_1','q83_2','q83_3','q83_4','q83_5','q83_6','q83_7','q83_8','q83_9','q84_1','q84_2','q84_3','q84_4','q84_5','q84_6','q84_7','q84_8','q84_9','q85_1','q85_2','q85_3','q85_4','q85_5','q85_6','q85_7','q85_8','q85_9','q86_1','q86_2','q86_3','q86_4','q86_5','q86_6','q86_7','q86_8','q86_9','q87_1','q87_2','q87_3','q87_4','q87_5','q87_6','q87_7','q87_8','q87_9','q88_1','q88_2','q88_3',\n",
    "'q88_4','q88_5','q88_6','q88_7','q88_8','q88_9','q89_1','q89_2','q89_3','q89_4','q89_5','q89_6','q89_7','q89_8','q89_9','q90_1','q90_2','q90_3','q90_4','q90_5','q90_6','q90_7','q90_8','q90_9','q91_1','q91_2','q91_3','q91_4','q91_5','q91_6','q91_7','q91_8','q91_9','q92_1','q92_2','q92_3','q92_4','q92_5','q92_6','q92_7','q92_8','q92_9','q93_1','q93_2','q93_3','q93_4','q93_5','q93_6','q93_7','q93_8','q93_9','q94_1','q94_2','q94_3','q94_4','q94_5','q94_6','q94_7','q94_8','q94_9','q95_1','q95_2','q95_3','q95_4',\n",
    "'q95_5','q95_6','q95_7','q9 5_8','q95_9']] \n",
    " \n",
    "df_predictions \n",
    "y_true = pd.read_csv(r'C:\\Users\\USER\\Downloads\\df.csv')\n",
    "y_true.index = pd.to_datetime(y_true['DATE'],format='%Y-%m-%d')\n",
    "y_true.drop('DATE',axis=1,inplace=True)\n",
    " \n",
    "N_Q = 9 \n",
    "k1 = 0 # 0 or 11\n",
    "k2 = 95 # 95 or 80 \n",
    "output_dim = k2 - k1 + 1 \n",
    "model_name = 'RFG' \n",
    "n_estimators = 500 \n",
    "max_depth = 5 \n",
    "learning_rate = 1e-2 \n",
    "VS_days = 11 \n",
    "q_set = np.array([i / (N_Q+1) for i in range(1, N_Q + 1)]) \n",
    "\n",
    "# Set of quantiles\n",
    "def crps_nrg_k(y_true:float, y_quantiles:np.array): \n",
    "    \"\"\" \n",
    "    Compute the CRPS NRG for a given leadtime k. \n",
    "    :param y_true: true value for this leadtime. \n",
    "    :param y_quantiles: quantile predictions for this leadtime with shape(N_Q,) \n",
    "    \"\"\" \n",
    "    N_Q = y_quantiles.shape[0] # Nb of quantiles predicted.    \n",
    "    simple_sum = np.sum(np.abs(y_quantiles - y_true)) / N_Q     \n",
    "    double_somme = 0 \n",
    "    for i in range(N_Q):         \n",
    "        for j in range(N_Q):            \n",
    "            double_somme += np.abs(y_quantiles[i] - y_quantiles[j])    \n",
    "            double_sum = double_somme / (2 * N_Q * N_Q) \n",
    " \n",
    "    crps = simple_sum  - double_sum \n",
    "return crps \n",
    "\n",
    " \n",
    "def crps_over_vs(df_pred:pd.DataFrame, df_true:pd.DataFrame, output_dim:int, N_Q:int, k1:int, k2:int): \n",
    "    \"\"\" \n",
    "    Compute the average of the CRPS over the validation set. \n",
    "    :param df_pred: quantile predictions (n_days, nb_quantiles * nb_forecasting_periods). \n",
    "    :param df_true: targets (n_days, nb_forecasting_periods).. \n",
    "    :param output_dim: nb_forecasting_periods. \n",
    "    :param N_Q: number of quantiles. \n",
    "    :param k1: first forecasting period. -> 0 <= k1 <= 95 \n",
    "    :param k2: last forecasting period. -> 0 <= k2 <= 95 and k1 <= k2. \n",
    "    \"\"\" \n",
    " \n",
    "    crps_list = []    \n",
    "    for leadtime in range(0, output_dim): \n",
    "        crps_k = 0         \n",
    "        for day in df_pred.index: \n",
    "            df_forecasts_dad_day = df_pred.loc[day].values.reshape(output_dim, N_Q) \n",
    "            crps_k += crps_nrg_k(y_true=df_true.loc[day].iloc[leadtime], y_quantiles=df_forecasts_dad_day[leadtime,:])         \n",
    "            crps_k = crps_k / len(df_pred.index)        \n",
    "            crps_list.append(crps_k) \n",
    "    df_crps = pd.DataFrame(data=crps_list, index=[i for i in range(k1, k2+1)], columns=['CRPS']) / 50 * 100 \n",
    "return df_crps \n",
    "\n",
    "import os\n",
    "# ------------------------------------------------------------------------------------------------------------------ \n",
    "# COMPUTE CRPS \n",
    "# ------------------------------------------------------------------------------------------------------------------ \n",
    "df_crps = crps_over_vs(df_pred=df_predictions, df_true=y_true, output_dim=output_dim, N_Q=N_Q, k1=k1, k2=k2) \n",
    "print(df_crps.mean()) \n",
    " \n",
    "dirname = r'C:\\Users\\USER\\Desktop\\J.DUMAS' \n",
    "if not os.path.isdir(dirname):  # test if directory exist     \n",
    "    os.makedirs(dirname) \n",
    "    df_crps.to_csv(dirname + 'quantile_scores_' + model_name + '_' + str(k1) + '_' + str(k2) + '.csv') \n",
    "\n",
    "    \n",
    "    \n",
    "    FONTSIZE = 20 \n",
    "    plt.figure() \n",
    "    plt.plot(df_crps.index, df_crps.values, linewidth=3, color='b', label='CRPS') \n",
    "    plt.ylim(0, 25) \n",
    "    plt.ylabel('%', fontsize=FONTSIZE, rotation='horizontal') \n",
    "    plt.xticks(fontsize=FONTSIZE)\n",
    "    plt.yticks(fontsize=FONTSIZE) \n",
    "    plt.legend(fontsize=FONTSIZE) \n",
    "    plt.tight_layout() \n",
    "    plt.savefig(dirname + 'quantile_scores_' + model_name + '_' + str(k1) + '_' + str(k2) + '.pdf') plt.close('all') \n",
    " \n",
    " \n",
    "# ------------------------------------------------------------------------------------------------------------------ \n",
    "# PLOTS \n",
    "# ------------------------------------------------------------------------------------------------------------------ \n",
    " \n",
    "df_point = pd.read_csv('export/'+model_name+'/forecasts/' + 'dad_point_' + model_name + '_' + str(k1) + '_' + str(k2) + '.csv', parse_dates=True, index_col=0) \n",
    " \n",
    "# Create folder \n",
    "dirname = r'C:\\Users\\USER\\Desktop\\J.DUMAS' \n",
    "if not os.path.isdir(dirname):  # test if directory exist    \n",
    "    os.makedirs(dirname) \n",
    "x_index = [i for i in range(k1, k2+1)] for day in df_predictions.index: \n",
    "    df_forecasts_day = pd.DataFrame(data=df_predictions.loc[day].values.reshape(output_dim,N_Q), index=[i for i in range(k1, k2+1)])     \n",
    "    FONTSIZE = 20 \n",
    "    plt.figure()     \n",
    "    for j in range(1, N_Q // 2): \n",
    "        plt.fill_between(x_index, df_forecasts_day[j + N_Q // 2].values, \n",
    "                         df_forecasts_day[(N_Q // 2) - j].values, \n",
    "                         alpha=0.5 / j, color=(1 / j, 0, 1))     \n",
    "        plt.plot(x_index, df_forecasts_day[0].values, 'b', linewidth=3, label='$q_1=$' + str(q_set[0])) \n",
    "        plt.plot(x_index, df_forecasts_day[N_Q-1].values, 'b', linewidth=3, label='$q_Q=$' + str(q_set[N_Q - 1]))     \n",
    "        plt.plot(x_index, y_true.loc[day].values, 'r', linewidth=3, label='Pm')     \n",
    "        plt.plot(x_index, df_point.loc[day].values, 'k', linewidth=3, label='Pp')     \n",
    "        plt.ylim(0, 50)     \n",
    "        plt.ylabel('MW', fontsize=FONTSIZE, rotation='horizontal')    \n",
    "        plt.xticks(fontsize=FONTSIZE)     \n",
    "        plt.yticks(fontsize=FONTSIZE) \n",
    "        plt.legend(fontsize=FONTSIZE) \n",
    "        plt.tight_layout() \n",
    "        plt.savefig(dirname + 'quantile_' + day.strftime('%Y%m%d') + '_' + model_name + '_' + str(k1) + '_' + str(k2) + '.pdf') \n",
    "        plt.close('all') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea94460",
   "metadata": {},
   "source": [
    "# Case 4: Gradient Boosting – Probabilistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6882d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble\n",
    "import GradientBoostingRegressor \n",
    "from sklearn.model_selection\n",
    "import train_test_split \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score, \n",
    " \n",
    "#train test split \n",
    "x_train, y_train = Final_data[['WIND DIRECTION','PANEL TEMPERATURE','GLOBAL IRRADIATION', 'AMBIENT TEMPERATURE','DIRECT IRRADIANCE','WIND SPEED']][:32448],Final_data.POWER[:32448] \n",
    "x_test, y_test =  Final_data[['WIND DIRECTION','PANEL TEMPERATURE','GLOBAL IRRADIATION','AMBIENT TEMPERATURE','DIRECT IRRADIANCE','WIND SPEED']][32448:],Final_data.POWER[32448:] # In[34]: \n",
    "x_train.shape, y_train.shape, x_test.shape,y_test.shape\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler() \n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.fit_transform(x_test) \n",
    "x_test_scaled; classifiers = {} for tau in [0.1,0.5,0.9]:\n",
    "clf = GradientBoostingRegressor(loss='quantile',alpha=tau) \n",
    "clf.fit(x_train,y_train)     \n",
    "preds = pd.DataFrame(clf.predict(x_test),columns=[str(tau)])    \n",
    "classifiers[str(tau)]={'clf': clf, 'predictions':preds} \n",
    "r2_score(preds,y_test) \n",
    "probdata = pd.DataFrame({'0.1':classifiers['0.1']['predictions']['0.1'], \n",
    "                     '0.5':classifiers['0.5']['predictions']['0.5'], \n",
    "                     '0.9':classifiers['0.9']['predictions']['0.9'],\n",
    "                    'actual_values':y_test.reset_index()['POWER']}) \n",
    "probdata[26:74] \n",
    "probdata.plot(figsize = (20,6)) \n",
    "plt.figure() \n",
    "probdata['0.1'].plot(figsize = (20,10)) \n",
    "probdata['0.9'].plot() \n",
    "probdata['0.5'].plot() \n",
    "\n",
    "df_VS_targets.to_csv('df.csv')\n",
    "\n",
    "\n",
    "def crps_over_vs(df_pred:pd.DataFrame, df_true:pd.DataFrame, output_dim:int, N_Q:int, k1:int, k2:int): \n",
    "    \"\"\" \n",
    "    Compute the average of the CRPS over the validation set. \n",
    "    :param df_pred: quantile predictions (n_days, nb_quantiles * nb_forecasting_periods). \n",
    "    :param df_true: targets (n_days, nb_forecasting_periods).. \n",
    "    :param output_dim: nb_forecasting_periods. \n",
    "    :param N_Q: number of quantiles. \n",
    "    :param k1: first forecasting period. -> 0 <= k1 <= 95 \n",
    "    :param k2: last forecasting period. -> 0 <= k2 <= 95 and k1 <= k2. \n",
    "    \"\"\" \n",
    " \n",
    "    crps_list = []     \n",
    "    for leadtime in range(0, output_dim): \n",
    "        crps_k = 0         \n",
    "        for day in df_pred.index: \n",
    "            df_forecasts_dad_day = df_pred.loc[day].values.reshape(output_dim, N_Q) \n",
    "            crps_k += crps_nrg_k(y_true=df_true.loc[day].iloc[leadtime], y_quantiles=df_forecasts_dad_day[leadtime,:])         \n",
    "            crps_k = crps_k / len(df_pred.index)         \n",
    "            crps_list.append(crps_k) \n",
    "    df_crps = pd.DataFrame(data=crps_list, index=[i for i in range(k1, k2+1)], columns=['CRPS']) / 50 * 100 \n",
    "return df_crps \n",
    "\n",
    "a_list = list(range(0,96))  \n",
    "df_VS_targets_R = df_VS_targets.copy() \n",
    "df_VS_targets_R.columns=[[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,2 5,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,\n",
    "                          45,46,47,48,49,50,51,52,53,54,55,5 6,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,8 7,88,89,90,91,92,93,94,95]] \n",
    "df_VS_targets \n",
    " \n",
    "df_VS_targets_R.to_csv('y_true.csv')\n",
    "\n",
    "import os \n",
    "\n",
    "def crps_nrg_k(y_true:float, y_quantiles:np.array): \n",
    "    \"\"\" \n",
    "    Compute the CRPS NRG for a given leadtime k. \n",
    "    :param y_true: true value for this leadtime. \n",
    "    :param y_quantiles: quantile predictions for this leadtime with shape(N_Q,) \n",
    "    \"\"\" \n",
    "    N_Q = y_quantiles.shape[0] # Nb of quantiles predicted.   \n",
    "    simple_sum = np.sum(np.abs(y_quantiles - y_true)) / N_Q     \n",
    "    double_somme = 0 \n",
    "    for i in range(N_Q):         \n",
    "        for j in range(N_Q): \n",
    "            double_somme += np.abs(y_quantiles[i] - y_quantiles[j]) \n",
    "            double_sum = double_somme / (2 * N_Q * N_Q) \n",
    "            crps = simple_sum  - double_sum \n",
    "return crps \n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------ \n",
    "# COMPUTE CRPS \n",
    "# ------------------------------------------------------------------------------------------------------------------ \n",
    "df_crps = crps_over_vs(df_pred=df_predictionsq, df_true=df_VS_targets_R, output_dim=output_dim, N_Q=N_Q, k1=k1, k2=k2) print(df_crps.mean()) \n",
    " \n",
    "dirname = r'C:\\Users\\USER\\Desktop\\Bui Masters Project\\PV data\\GBRProb\\New folder' \n",
    "if not os.path.isdir(dirname):  # test if directory exist     \n",
    "    os.makedirs(dirname) \n",
    "df_crps.to_csv(dirname + 'quantile_scores_' + model_name + '_' + str(k1) + '_' + str(k2) + '.csv') \n",
    " \n",
    "FONTSIZE = 20 \n",
    "plt.figure() \n",
    "plt.plot(df_crps.index, df_crps.values, linewidth=1, color='b', label='CRPS') \n",
    "plt.ylim(0, 25) \n",
    "plt.ylabel('%', fontsize=FONTSIZE, rotation='horizontal') \n",
    "plt.xticks(fontsize=FONTSIZE) \n",
    "plt.yticks(fontsize=FONTSIZE)\n",
    "plt.legend(fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig(dirname + 'quantile_scores_' + model_name + '_' + str(k1) + '_' + str(k2) + '.pdf') plt.close('all') # In[ ]: \n",
    "# ------------------------------------------------------------------------------------------------------------------ \n",
    "# PLOTS \n",
    "# ------------------------------------------------------------------------------------------------------------------ \n",
    " \n",
    "df_point = pd.read_csv('export/'+model_name+'/forecasts/' + 'dad_point_' + model_name + '_' + str(k1) + '_' + str(k2) + '.csv', parse_dates=True, index_col=0) \n",
    " \n",
    "# Create folder \n",
    "dirname = r'C:\\Users\\USER\\Desktop\\J.DUMAS' \n",
    "if not os.path.isdir(dirname):  # test if directory exist     \n",
    "    os.makedirs(dirname) \n",
    "\n",
    "    \n",
    "    \n",
    "x_index = [i for i in range(k1, k2+1)] \n",
    "for day in df_predictions.index: \n",
    "    df_forecasts_day = pd.DataFrame(data=df_predictions.loc[day].values.reshape(output_dim,N_Q), index=[i for i in range(k1, k2+1)])     \n",
    "    FONTSIZE = 20 \n",
    "    plt.figure()     \n",
    "    for j in range(1, N_Q // 2): \n",
    "        plt.fill_between(x_index, df_forecasts_day[j + N_Q // 2].values, df_forecasts_day[(N_Q // 2) - j].values, alpha=0.5 / j, color=(1 / j, 0, 1))     \n",
    "        plt.plot(x_index, df_forecasts_day[0].values, 'b', linewidth=3, label='$q_1=$' + str(q_set[0])) \n",
    "    plt.plot(x_index, df_forecasts_day[N_Q-1].values, 'b', linewidth=3, label='$q_Q=$' + str(q_set[N_Q - 1]))    \n",
    "    plt.plot(x_index, df_VS_targets_R.loc[day].values, 'r', linewidth=3, label='Pm')    \n",
    "    plt.plot(x_index, df_point.loc[day].values, 'k', linewidth=3, label='Pp')    \n",
    "    plt.ylim(0, 50)    \n",
    "    plt.ylabel('MW', fontsize=FONTSIZE, rotation='horizontal')     \n",
    "    plt.xticks(fontsize=FONTSIZE)     \n",
    "    plt.yticks(fontsize=FONTSIZE)    \n",
    "    plt.legend(fontsize=FONTSIZE) \n",
    "    plt.tight_layout() \n",
    "    plt.savefig(dirname + 'quantile_' + day.strftime('%Y%m%d') + '_' + model_name + '_' + str(k1) + '_' + str(k2) + '.pdf')    \n",
    "    plt.close('all') \n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
